#!/usr/bin/env python3
"""
Readability Checker - Analisa legibilidade de textos

M√©tricas:
- Flesch Reading Ease (adaptado para portugu√™s)
- Tempo de leitura estimado
- Complexidade de vocabul√°rio
- Estrutura de senten√ßas

Uso:
    python readability_checker.py "Seu texto aqui"
    python readability_checker.py --file artigo.txt
    python readability_checker.py --url https://exemplo.com/artigo
"""

import re
import sys
import argparse
from typing import Dict, List, Tuple
from collections import Counter

# Conectivos e palavras de transi√ß√£o (bom sinal)
TRANSITION_WORDS = [
    "portanto", "por√©m", "contudo", "entretanto", "todavia",
    "al√©m disso", "ademais", "por outro lado", "em contrapartida",
    "por exemplo", "ou seja", "isto √©", "em outras palavras",
    "em resumo", "em suma", "finalmente", "primeiramente",
    "em seguida", "posteriormente", "consequentemente",
    "assim", "dessa forma", "desse modo", "sendo assim"
]

# Palavras dif√≠ceis comuns que podem ser simplificadas
COMPLEX_WORDS = {
    "utilizar": "usar",
    "realizar": "fazer",
    "efetuar": "fazer",
    "implementar": "criar/fazer",
    "otimizar": "melhorar",
    "potencializar": "aumentar",
    "viabilizar": "permitir",
    "disponibilizar": "oferecer",
    "priorizar": "dar prioridade",
    "customizar": "personalizar"
}

# Jarg√µes a evitar
JARGON_WORDS = [
    "sinergia", "paradigma", "disruptivo", "escal√°vel",
    "hol√≠stico", "proativo", "stakeholder", "mindset",
    "benchmark", "brainstorm", "deadline", "feedback",
    "insight", "expertise", "networking", "approach"
]


def count_syllables_pt(word: str) -> int:
    """Conta s√≠labas em portugu√™s (aproxima√ß√£o)."""
    word = word.lower()
    vowels = "aeiou√°√©√≠√≥√∫√¢√™√¥√£√µ"
    count = 0
    prev_was_vowel = False

    for char in word:
        is_vowel = char in vowels
        if is_vowel and not prev_was_vowel:
            count += 1
        prev_was_vowel = is_vowel

    # Ajustes para ditongos e hiatos comuns
    # Ditongos decrescentes (contam como 1 s√≠laba)
    ditongos = ["ai", "ei", "oi", "ui", "au", "eu", "ou", "iu"]
    for d in ditongos:
        if d in word:
            count -= word.count(d)
            count += word.count(d)  # Mant√©m contagem

    return max(1, count)


def get_sentences(text: str) -> List[str]:
    """Divide texto em senten√ßas."""
    # Divide por pontua√ß√£o final
    sentences = re.split(r'[.!?]+', text)
    # Remove vazios e limpa espa√ßos
    return [s.strip() for s in sentences if s.strip()]


def get_words(text: str) -> List[str]:
    """Extrai palavras do texto."""
    # Remove pontua√ß√£o e divide
    words = re.findall(r'\b[a-z√°√©√≠√≥√∫√¢√™√¥√£√µ√ß]+\b', text.lower())
    return words


def calculate_flesch_pt(text: str) -> Tuple[float, str]:
    """
    Calcula Flesch Reading Ease adaptado para portugu√™s.
    F√≥rmula: 248.835 - (1.015 √ó ASL) - (84.6 √ó ASW)
    ASL = Average Sentence Length
    ASW = Average Syllables per Word
    """
    sentences = get_sentences(text)
    words = get_words(text)

    if not sentences or not words:
        return 0, "Texto muito curto"

    # M√©dia de palavras por senten√ßa
    asl = len(words) / len(sentences)

    # M√©dia de s√≠labas por palavra
    total_syllables = sum(count_syllables_pt(w) for w in words)
    asw = total_syllables / len(words)

    # F√≥rmula Flesch adaptada para portugu√™s
    score = 248.835 - (1.015 * asl) - (84.6 * asw)
    score = max(0, min(100, score))

    # Interpreta√ß√£o
    if score >= 80:
        level = "Muito f√°cil (5¬∫ ano)"
    elif score >= 70:
        level = "F√°cil (6¬∫-7¬∫ ano)"
    elif score >= 60:
        level = "Moderado (8¬∫-9¬∫ ano)"
    elif score >= 50:
        level = "M√©dio (Ensino M√©dio)"
    elif score >= 30:
        level = "Dif√≠cil (Superior)"
    else:
        level = "Muito dif√≠cil (Acad√™mico)"

    return round(score, 1), level


def calculate_reading_time(text: str) -> Dict:
    """Calcula tempo de leitura estimado."""
    words = get_words(text)
    word_count = len(words)

    # Velocidades m√©dias de leitura
    speeds = {
        "leitura_rapida": 300,  # palavras por minuto
        "leitura_normal": 200,
        "leitura_atenta": 150
    }

    times = {}
    for speed_name, wpm in speeds.items():
        minutes = word_count / wpm
        if minutes < 1:
            times[speed_name] = f"{int(minutes * 60)} segundos"
        else:
            times[speed_name] = f"{round(minutes, 1)} minutos"

    return {
        "word_count": word_count,
        "times": times
    }


def analyze_sentences(text: str) -> Dict:
    """Analisa estrutura das senten√ßas."""
    sentences = get_sentences(text)

    if not sentences:
        return {"error": "Sem senten√ßas para analisar"}

    lengths = [len(get_words(s)) for s in sentences]

    analysis = {
        "total_sentences": len(sentences),
        "avg_length": round(sum(lengths) / len(lengths), 1),
        "shortest": min(lengths),
        "longest": max(lengths),
        "very_long": sum(1 for l in lengths if l > 25),  # Senten√ßas muito longas
        "very_short": sum(1 for l in lengths if l < 5),  # Senten√ßas muito curtas
    }

    # Varia√ß√£o (bom ter mix)
    if len(lengths) > 1:
        import statistics
        analysis["std_dev"] = round(statistics.stdev(lengths), 1)
        if analysis["std_dev"] > 5:
            analysis["variation"] = "‚úÖ Boa varia√ß√£o"
        else:
            analysis["variation"] = "‚ö†Ô∏è Pouca varia√ß√£o"
    else:
        analysis["variation"] = "N/A"

    return analysis


def find_transition_words(text: str) -> List[str]:
    """Encontra palavras de transi√ß√£o."""
    text_lower = text.lower()
    found = []

    for word in TRANSITION_WORDS:
        if word in text_lower:
            found.append(word)

    return found


def find_complex_words(text: str) -> List[Tuple[str, str]]:
    """Encontra palavras complexas e sugere alternativas."""
    text_lower = text.lower()
    found = []

    for complex_word, simple in COMPLEX_WORDS.items():
        if complex_word in text_lower:
            found.append((complex_word, simple))

    return found


def find_jargon(text: str) -> List[str]:
    """Encontra jarg√µes no texto."""
    text_lower = text.lower()
    return [j for j in JARGON_WORDS if j in text_lower]


def analyze_paragraphs(text: str) -> Dict:
    """Analisa estrutura de par√°grafos."""
    paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]

    if not paragraphs:
        paragraphs = [p.strip() for p in text.split('\n') if p.strip()]

    if not paragraphs:
        return {"error": "Sem par√°grafos"}

    lengths = [len(get_words(p)) for p in paragraphs]

    return {
        "total": len(paragraphs),
        "avg_words": round(sum(lengths) / len(lengths), 1),
        "ideal_range": "50-150 palavras por par√°grafo",
        "too_long": sum(1 for l in lengths if l > 150),
        "too_short": sum(1 for l in lengths if l < 20)
    }


def get_vocabulary_stats(text: str) -> Dict:
    """Estat√≠sticas de vocabul√°rio."""
    words = get_words(text)

    if not words:
        return {"error": "Sem palavras"}

    word_freq = Counter(words)
    unique_words = len(word_freq)

    # Lexical diversity (type-token ratio)
    ttr = unique_words / len(words)

    # Palavras longas (4+ s√≠labas)
    long_words = [w for w in words if count_syllables_pt(w) >= 4]

    return {
        "total_words": len(words),
        "unique_words": unique_words,
        "lexical_diversity": round(ttr, 3),
        "long_words_count": len(long_words),
        "long_words_percent": round(len(long_words) / len(words) * 100, 1),
        "most_common": word_freq.most_common(10)
    }


def full_analysis(text: str) -> Dict:
    """An√°lise completa de legibilidade."""
    results = {
        "text_preview": text[:200] + "..." if len(text) > 200 else text
    }

    # Flesch
    flesch_score, flesch_level = calculate_flesch_pt(text)
    results["flesch"] = {
        "score": flesch_score,
        "level": flesch_level
    }

    # Tempo de leitura
    results["reading_time"] = calculate_reading_time(text)

    # Senten√ßas
    results["sentences"] = analyze_sentences(text)

    # Par√°grafos
    results["paragraphs"] = analyze_paragraphs(text)

    # Vocabul√°rio
    results["vocabulary"] = get_vocabulary_stats(text)

    # Transi√ß√µes
    results["transitions"] = find_transition_words(text)

    # Palavras complexas
    results["complex_words"] = find_complex_words(text)

    # Jarg√µes
    results["jargon"] = find_jargon(text)

    # Score geral (0-100)
    score = flesch_score * 0.4  # 40% flesch

    # Bonus por transi√ß√µes
    if results["transitions"]:
        score += min(10, len(results["transitions"]) * 2)

    # Penalidade por senten√ßas longas
    if results["sentences"].get("very_long", 0) > 0:
        score -= results["sentences"]["very_long"] * 3

    # Penalidade por jarg√µes
    score -= len(results["jargon"]) * 2

    results["overall_score"] = round(max(0, min(100, score)), 1)

    # Classifica√ß√£o
    if results["overall_score"] >= 70:
        results["classification"] = "‚úÖ Excelente legibilidade"
    elif results["overall_score"] >= 50:
        results["classification"] = "üëç Boa legibilidade"
    elif results["overall_score"] >= 30:
        results["classification"] = "‚ö†Ô∏è Legibilidade m√©dia"
    else:
        results["classification"] = "‚ùå Dif√≠cil de ler"

    return results


def print_report(results: Dict):
    """Imprime relat√≥rio formatado."""
    print("\n" + "="*60)
    print("üìñ AN√ÅLISE DE LEGIBILIDADE")
    print("="*60)

    print(f"\nüéØ SCORE GERAL: {results['overall_score']}/100")
    print(f"   {results['classification']}")

    print("\n" + "-"*60)
    print("üìä M√âTRICAS PRINCIPAIS")
    print("-"*60)

    # Flesch
    print(f"\nüìà √çndice Flesch: {results['flesch']['score']}")
    print(f"   N√≠vel: {results['flesch']['level']}")

    # Tempo de leitura
    rt = results['reading_time']
    print(f"\n‚è±Ô∏è Tempo de Leitura ({rt['word_count']} palavras):")
    print(f"   ‚Ä¢ Leitura r√°pida: {rt['times']['leitura_rapida']}")
    print(f"   ‚Ä¢ Leitura normal: {rt['times']['leitura_normal']}")
    print(f"   ‚Ä¢ Leitura atenta: {rt['times']['leitura_atenta']}")

    # Senten√ßas
    sent = results['sentences']
    if "error" not in sent:
        print(f"\nüìù Senten√ßas ({sent['total_sentences']} total):")
        print(f"   ‚Ä¢ M√©dia de palavras: {sent['avg_length']}")
        print(f"   ‚Ä¢ Mais curta: {sent['shortest']} | Mais longa: {sent['longest']}")
        if sent['very_long'] > 0:
            print(f"   ‚ö†Ô∏è {sent['very_long']} senten√ßas muito longas (>25 palavras)")
        print(f"   ‚Ä¢ Varia√ß√£o: {sent['variation']}")

    # Par√°grafos
    para = results['paragraphs']
    if "error" not in para:
        print(f"\nüìÑ Par√°grafos ({para['total']} total):")
        print(f"   ‚Ä¢ M√©dia: {para['avg_words']} palavras")
        print(f"   ‚Ä¢ Ideal: {para['ideal_range']}")

    # Vocabul√°rio
    vocab = results['vocabulary']
    if "error" not in vocab:
        print(f"\nüìö Vocabul√°rio:")
        print(f"   ‚Ä¢ Palavras √∫nicas: {vocab['unique_words']} de {vocab['total_words']}")
        print(f"   ‚Ä¢ Diversidade lexical: {vocab['lexical_diversity']}")
        print(f"   ‚Ä¢ Palavras longas (4+ s√≠labas): {vocab['long_words_percent']}%")

    print("\n" + "-"*60)
    print("üí° AN√ÅLISE QUALITATIVA")
    print("-"*60)

    # Transi√ß√µes
    if results['transitions']:
        print(f"\n‚úÖ Conectivos encontrados ({len(results['transitions'])}):")
        print(f"   {', '.join(results['transitions'][:8])}")
    else:
        print("\n‚ö†Ô∏è Nenhum conectivo/palavra de transi√ß√£o encontrado")
        print("   Considere adicionar: portanto, al√©m disso, por exemplo...")

    # Palavras complexas
    if results['complex_words']:
        print(f"\n‚ö†Ô∏è Palavras que podem ser simplificadas:")
        for complex_w, simple in results['complex_words'][:5]:
            print(f"   ‚Ä¢ '{complex_w}' ‚Üí '{simple}'")

    # Jarg√µes
    if results['jargon']:
        print(f"\n‚ö†Ô∏è Jarg√µes encontrados:")
        print(f"   {', '.join(results['jargon'])}")
        print("   Considere explicar ou substituir por termos mais simples")

    print("\n" + "-"*60)
    print("üìã RECOMENDA√á√ïES")
    print("-"*60)

    recommendations = []

    if results['flesch']['score'] < 50:
        recommendations.append("Simplifique o texto: use frases mais curtas e palavras mais simples")

    if sent.get('very_long', 0) > 2:
        recommendations.append("Quebre senten√ßas longas em duas ou mais")

    if not results['transitions']:
        recommendations.append("Adicione conectivos para melhorar a fluidez")

    if vocab.get('long_words_percent', 0) > 20:
        recommendations.append("Reduza o uso de palavras longas/t√©cnicas")

    if results['jargon']:
        recommendations.append("Substitua jarg√µes por termos mais acess√≠veis")

    if para.get('too_long', 0) > 0:
        recommendations.append("Divida par√°grafos muito longos")

    if not recommendations:
        recommendations.append("Texto bem escrito! Mantenha o bom trabalho.")

    for i, rec in enumerate(recommendations, 1):
        print(f"\n{i}. {rec}")

    print("\n" + "="*60)


def main():
    parser = argparse.ArgumentParser(description="Analisa legibilidade de textos")
    parser.add_argument("text", nargs="*", help="Texto para analisar")
    parser.add_argument("--file", "-f", help="Arquivo de texto")

    args = parser.parse_args()

    text = ""

    if args.file:
        with open(args.file, 'r', encoding='utf-8') as f:
            text = f.read()
    elif args.text:
        text = " ".join(args.text)
    else:
        print("Uso: python readability_checker.py \"Seu texto aqui\"")
        print("     python readability_checker.py --file artigo.txt")
        sys.exit(1)

    if len(text) < 50:
        print("‚ùå Texto muito curto para an√°lise significativa (m√≠nimo 50 caracteres)")
        sys.exit(1)

    results = full_analysis(text)
    print_report(results)


if __name__ == "__main__":
    main()
